# Fine-tuning Guide: Train GPT-4o-mini with Clinic Knowledge

## Overview
Fine-tuning ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ AI ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å‡∏ù‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á context ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á

## Benefits
- ‚úÖ ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î tokens ~70-80% (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á knowledge context)
- ‚úÖ ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô (context ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á)
- ‚úÖ AI ‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏ß‡πà‡∏≤ (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ù‡∏±‡∏á‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•)
- ‚úÖ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö knowledge ‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô

## Cost Estimate
- Training: ~$3-8 USD (‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô knowledge)
- API usage: ‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° ($0.15/$0.60 per 1M tokens)
- ROI: ‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô >1000 messages/month

## Steps

### 1. Prepare Training Data
```bash
npx tsx scripts/prepare-finetune-data.ts
```

Output: `finetune-data.jsonl` (~20-50 KB)

### 2. Upload & Start Training
```bash
npx tsx scripts/upload-finetune.ts
```

‡∏à‡∏∞‡πÑ‡∏î‡πâ Job ID ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞

### 3. Monitor Progress (1-2 hours)
```bash
# Check specific job
npx tsx scripts/check-finetune-status.ts <job-id>

# List all jobs
npx tsx scripts/check-finetune-status.ts
```

### 4. Use Fine-tuned Model

‡πÄ‡∏°‡∏∑‡πà‡∏≠ training ‡πÄ‡∏™‡∏£‡πá‡∏à ‡∏à‡∏∞‡πÑ‡∏î‡πâ model name ‡πÄ‡∏ä‡πà‡∏ô: `ft:gpt-4o-mini-2024-07-18:org:custom:abc123`

‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô `lib/ai/agent.ts`:
```typescript
const completion = await openaiClient.chat.completions.create({
  model: 'ft:gpt-4o-mini-2024-07-18:org:custom:abc123', // ‚Üê ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
  messages: [
    { role: 'system', content: SYSTEM_PROMPT },
    { role: 'user', content: userMessage }
  ],
  temperature: 0.7,
  max_tokens: 500,
});
```

**‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á context ‡πÅ‡∏•‡πâ‡∏ß!** AI ‡∏£‡∏π‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß üéâ

## Testing

‡∏´‡∏•‡∏±‡∏á fine-tune ‡πÄ‡∏™‡∏£‡πá‡∏à ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏î‡∏¢:
1. Deploy code ‡πÉ‡∏´‡∏°‡πà
2. ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Hutox, ‡∏£‡∏≤‡∏Ñ‡∏≤, ‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô
3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö token usage (‡∏Ñ‡∏ß‡∏£‡∏•‡∏î‡∏•‡∏á 70-80%)

## Re-training

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ knowledge ‡πÉ‡∏´‡∏°‡πà:
```bash
# 1. Re-export data
npx tsx scripts/prepare-finetune-data.ts

# 2. Upload & train
npx tsx scripts/upload-finetune.ts

# 3. Update model name ‡πÉ‡∏ô agent.ts
```

## Notes
- Fine-tuned model ‡∏à‡∏∞‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏‡∏´‡∏•‡∏±‡∏á 90 ‡∏ß‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
- ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ train ‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î
- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ re-train ‡∏ó‡∏∏‡∏Å 1-2 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
